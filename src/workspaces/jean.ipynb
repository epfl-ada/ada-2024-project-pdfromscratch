{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba_beers     = pd.read_csv('../data/beer_advocate/beers.csv')\n",
    "df_ba_breweries = pd.read_csv('../data/beer_advocate/breweries.csv')\n",
    "df_ba_users     = pd.read_csv('../data/beer_advocate/users.csv')\n",
    "df_ba_ratings   = pd.read_csv('../data/beer_advocate/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer Advocate - Trends Analysis (Question 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba_ratings['date_week'] = pd.to_datetime(df_ba_ratings['date'], unit=\"s\").dt.to_period('W')\n",
    "df_ba_ratings['date_day']  = pd.to_datetime(df_ba_ratings['date'], unit=\"s\").dt.round('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "df_ba_ratings.groupby('date_week').size().plot(\n",
    "    title='Number of ratings per week',\n",
    "    xlabel='Week',\n",
    "    ylabel='Number of ratings',\n",
    "    ax=axes[0]\n",
    ")\n",
    "df_ba_ratings[\n",
    "    (df_ba_ratings['date_week'] >= '2011-01-01') &\n",
    "    (df_ba_ratings['date_week'] <= '2013-01-01')\n",
    "].groupby('date_week').size().plot(\n",
    "    title='Number of ratings per week',\n",
    "    xlabel='Week',\n",
    "    ylabel='Number of ratings',\n",
    "    ax=axes[1]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that something happended around November 2011. Indeed, there is a period spanning approximately 45 days with a very high number of ratings compared to usual regime. After performing some research we were not able to find the cause of this peak. We decided to focus on the data covering the period after November 2011 since it represents ~80% of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE_START = '2011-11-14'\n",
    "CUTOFF_DATE_END = '2012-01-01'\n",
    "\n",
    "print((df_ba_ratings['date_day'] >= CUTOFF_DATE_END).mean())\n",
    "\n",
    "stats.ttest_ind(\n",
    "    df_ba_ratings[df_ba_ratings['date_day'] < CUTOFF_DATE_START]['rating'],\n",
    "    df_ba_ratings[df_ba_ratings['date_day'] >= CUTOFF_DATE_END]['rating']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba_ratings_trend = df_ba_ratings[df_ba_ratings['date_day'] >= CUTOFF_DATE_END].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ba_ratings_trend.groupby('date_week').size().plot(\n",
    "    title='Number of ratings per week',\n",
    "    xlabel='Week',\n",
    "    ylabel='Number of ratings'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe covering all days on the analysed period\n",
    "df_dates = pd.DataFrame(index=pd.date_range(\n",
    "    start=pd.to_datetime(CUTOFF_DATE_END).round('D'),\n",
    "    end=pd.to_datetime(df_ba_ratings_trend['date'].max(), unit='s').round('D'),\n",
    "freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe containing the number of ratings per day for each style (with missing days)\n",
    "df_partial_time_series = df_ba_ratings_trend.groupby(['date_day', 'beer_global_style'])\\\n",
    "    .size()\\\n",
    "    .reset_index(level=1, name='count')\\\n",
    "    .pivot(columns='beer_global_style', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging and filling NA with 0 in order to have full time series for each style\n",
    "df_time_series = pd.merge(\n",
    "    df_dates,\n",
    "    df_partial_time_series,\n",
    "    how='left', left_index=True, right_index=True\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition\n",
    "\n",
    "Using the moving averages and assuming that the observed time serie $O$ can be decomposed in an additive way in $O = T + S + R$ where $T$ is the general trend, $S$ the seasonal effect and $R$ the residuals effects, we can split the time serie into those 3 components.\n",
    "\n",
    "The residuals $R$ are some sense the effect that cannot be explained by seasonality or general trend (e.g. popularity of BeerAdvocate, increasing beer market, etc.) and thus are interesting to detect trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_decompose(df_time_series['India Pale Ale'], model='additive', period=365).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = df_time_series.apply(lambda ts: seasonal_decompose(ts, model='additive', period=365).resid).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe `df_residuals` contains, for each beer style, the time serie of the residuals. In order to compare the residuals of different styles, we need to normalize the time series to compare meaningful values (Z-scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = (df_residuals - df_residuals.mean()) / df_residuals.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends identification - Approach 1\n",
    "\n",
    "Approach 1 is based on two criteria computed on a moving window :\n",
    "\n",
    "- **Intra variation** : Compute the Z-scores on the time serie for a given style\n",
    "- **Inter variation** : Compute the Z-scores of all styles on a given time period (window)\n",
    "\n",
    "The interpretation is that a high **intra** Z-score means that this style has a significantly high number of ratings compared to the usual number of ratings of this style. A high **inter** Z-score means that this style has a significantly high number of ratings compared to other styles during this time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hype_period(\n",
    "        df_residuals: pd.DataFrame, \n",
    "        style: str,\n",
    "        window_size: int = 14,\n",
    "        intra_threshold: float = 2,\n",
    "        inter_threshold: float = 2\n",
    "    ):\n",
    "\n",
    "    df_residuals_intra = (df_residuals.rolling(window=window_size).mean() - df_residuals.mean()) / df_residuals.std().dropna()\n",
    "    df_residuals_inter = df_residuals.rolling(window=window_size).mean().dropna().apply(stats.zscore, axis=1)\n",
    "\n",
    "    serie = df_residuals_intra[style]\n",
    "    values = serie[(df_residuals_intra[style] > intra_threshold) & (df_residuals_inter[style] > inter_threshold)]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title(f'[{style}] Normalized residuals (rolling average of {window_size} days) (intra z-score > {intra_threshold}, inter z-score > {inter_threshold})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized residuals')\n",
    "    plt.plot(serie, alpha=0.5)\n",
    "    plt.scatter(values.index, values, color='red', label='Hype period')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hype_period(df_residuals, 'India Pale Ale', window_size=7, intra_threshold=1.5, inter_threshold=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of each red dot (hype) :\n",
    "- **Intra** : the average number of ratings of the past 7 days was  1.5 standard deviation higher than the average number of ratings for this style during the entire timeframe of the dataset\n",
    "- **Inter** : the average number of ratings of the past 7 days was 1.5 standard deviation higher than the average number of ratings for all style during those 7 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends Identification - Approach 2\n",
    "\n",
    "Approach 2 keeps the criteria of intra and inter variation but rather than looking at moving averages, it analyze the number of days during which the constraint is respected. For example for the intra criteria, the Z-score should be higher than the threshold each day of the window (rather than the mean of the window as in approach 1). Approach 2 is therefore more strict than approach 1, but is also more sensitive to noise (e.g. hype period of 14 days, if one day, there is less rating, it will be missed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hype_period_2(\n",
    "        df_residuals: pd.DataFrame, \n",
    "        style: str,\n",
    "        window_size: int = 3,\n",
    "        intra_threshold: float = 2,\n",
    "        inter_threshold: float = 2\n",
    "    ):\n",
    "    serie = df_residuals[style]\n",
    "\n",
    "    df_residuals_intra = (serie - serie.mean()) / serie.std()\n",
    "    df_residuals_inter = df_residuals.apply(stats.zscore, axis=1)[style]\n",
    "\n",
    "    intra_condition = (df_residuals_intra > intra_threshold).rolling(window=window_size).apply(lambda x: x.all(), raw=True)\n",
    "    inter_condition = (df_residuals_inter > inter_threshold).rolling(window=window_size).apply(lambda x: x.all(), raw=True)\n",
    "\n",
    "    values = serie.where((intra_condition == 1) & (inter_condition == 1))\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title(f'[{style}] Normalized residuals (constraint window of {window_size} days) (intra z-score > {intra_threshold}, inter z-score > {inter_threshold})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized residuals')\n",
    "    plt.plot(serie, alpha=0.5)\n",
    "\n",
    "    plt.scatter(values.index, values, color='red', label='Hype period')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hype_period_2(df_residuals, 'Dark Lager', window_size=3, intra_threshold=1.5, inter_threshold=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation : intra and inter z-scores are greather than the threshold for 3 consecutive days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends Identification - Approach 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hype_period_3(\n",
    "        df_residuals: pd.DataFrame, \n",
    "        style: str,\n",
    "        window_size_pct_change: int = 3,\n",
    "        window_size_constraint: int = 14\n",
    "    ):\n",
    "    serie = df_residuals[style]\n",
    "\n",
    "    smoothed_changes = serie.pct_change().rolling(window=window_size_pct_change, center=True).mean()\n",
    "\n",
    "    changes_threshold = (0 - smoothed_changes.mean()) / smoothed_changes.std()\n",
    "    smoothed_changes = (smoothed_changes - smoothed_changes.mean()) / smoothed_changes.std()\n",
    "\n",
    "    changes_condition = (smoothed_changes > changes_threshold).rolling(window=window_size_constraint).apply(lambda x: x.all(), raw=True)\n",
    "    values = serie.where(changes_condition == 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title(f'[{style}] Normalized residuals (constraint window of {window_size_constraint} days) (changes computed on {window_size_pct_change} days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Normalized residuals')\n",
    "\n",
    "    plt.plot(serie, alpha=0.5)\n",
    "    plt.plot(smoothed_changes, alpha=0.5)\n",
    "    plt.scatter(values.index, values, color='red', label='Hype period')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hype_period_3(df_residuals, 'India Pale Ale', window_size_pct_change=5, window_size_constraint=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation : percentage change of the number of ratings average on a 5 days sliding windows is positive (increasing number of ratings) for 14 consecutives days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis (Question 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_MODEL = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HUGGING_FACE_MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(HUGGING_FACE_MODEL)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(reviews: pd.Series) -> pd.DataFrame:\n",
    "    encoded_input = tokenizer(reviews.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    encoded_input = {key: tensor.to(device) for key, tensor in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    scores = softmax(output.logits, dim=1)\n",
    "    return pd.DataFrame([\n",
    "        *scores.cpu().numpy()\n",
    "    ], columns=['1', '2', '3', '4', '5'], index=reviews.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = df_ba_ratings[df_ba_ratings['text'].str.len() > 377]['text'].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = predict_rating(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
